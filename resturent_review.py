# -*- coding: utf-8 -*-
"""Resturent_Review.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FZXBJlj0tnlWvll7wAL6zRD7GNcnOhk5
"""

from google.colab import files
import pandas as pd
uploaded=files.upload()
import io
df2 = pd.read_csv(io.BytesIO(uploaded['Restaurant_Reviews.tsv']),sep="\t")

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
all_words=[]
for sent in df2['Review']:
  tokenize_words=word_tokenize(sent)
  for words in tokenize_words:
    all_words.append(words)

# print(len(all_words))
unique_words=set(all_words)
# print(len(unique_words))
vocab_len=len(unique_words)
corpus = [
    # Positive Reviews

    'The test of food was Awesome I was totally amazed',
    'I love the test of their resturent',
    'Good service good food i love it',
    'Kadi and puri was awesome',
    'Wonderfully cooked and present every food',
    'Its a fantastic food I would like to come often and often',
    'Never ate such a brillent food',
    'It was awesmoe to have such good food',

    # Negtive Reviews

    "Need to work on their test",
    'waste food I have ever had',
    'pathetic service and food',
    'It was very disappointed with the service',
    'I did not like the amenities and food',
    'The service was horrible',
    'I will not recommend to go their',
    'The food and service is pathetic'
]
all_words1=[]
for sent in corpus:
  tokenize_words=word_tokenize(sent)
  for words in tokenize_words:
    all_words1.append(words)

# print(len(all_words1))
unique_words1=set(all_words1)
# print(len(unique_words1))
vocab_len1=len(unique_words1)

df2.head()

df2.shape

from keras.preprocessing.text import one_hot
embedded_sentences=[one_hot(sent,vocab_len) for sent in df2['Review']]
embedded_sentences2=[one_hot(sent,vocab_len1) for sent in corpus]
print(embedded_sentences2)

word_count=lambda sentence: len(word_tokenize(sentence))
long_sen=max(df2['Review'],key=word_count)
log_len_sen=len(word_tokenize(long_sen))
print(log_len_sen)
word_count=lambda sentence: len(word_tokenize(sentence))
long_sen1=max(corpus,key=word_count)
log_len_sen1=len(word_tokenize(long_sen1))
print(log_len_sen1)

from keras.preprocessing.sequence import pad_sequences
padded_sentences1=pad_sequences(embedded_sentences2,log_len_sen,padding='post')
padded_sentences=pad_sequences(embedded_sentences,log_len_sen,padding='post')
print(padded_sentences)

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(padded_sentences,df2['Liked'],test_size=0.25,random_state=100)

from keras.models import Sequential
from keras.layers import Embedding,Flatten,Dense,Dropout
from keras.callbacks import EarlyStopping,ModelCheckpoint
model=Sequential()
model.add(Embedding(vocab_len,50,input_length=log_len_sen))
model.add(Flatten())
model.add(Dense(vocab_len,activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(vocab_len,activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1,activation='sigmoid'))



model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])
es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)
mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)
model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=100,verbose=1,callbacks=[es,mc])
# print(model.summary())

import re
re.split('mum', 'mumbai*', 1)

loss,accuracy=model.evaluate(x_test,y_test,verbose=0)
print('Accuracy: %f'%(accuracy*100))

loss,accuracy=model.evaluate(padded_sentences,df2['Liked'],verbose=0)
print('Accuracy: %f'%(accuracy*100))

from google.colab import files
files.download('Resturent_Review.ipynb')

